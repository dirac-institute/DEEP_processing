# given an input collection and a pipeline
# execute the pipeline on the inputs and place the outputs into a run collection:
# {input}/{pipeline_name}/{pipeline_step}/{datetime}

from datetime import datetime
import re
import os
import lsst.daf.butler as dafButler
from lsst.daf.butler.registry import CollectionType
import atexit
from subprocess import Popen, PIPE
import selectors
import sys
import argparse

processes = []

def cleanup():
    for p in processes:
        p.kill()

atexit.register(cleanup)

def popen(*args, **kwargs):
    global processes
    p = Popen(*args, **kwargs)
    print("popen: " + " ".join(p.args), file=sys.stderr)
    processes.append(p)
    return p

def _print(p):
    sel = selectors.DefaultSelector()
    sel.register(p.stdout, selectors.EVENT_READ)
    sel.register(p.stderr, selectors.EVENT_READ)

    while True:
        for key, _ in sel.select():
            data = key.fileobj.read1().decode()
            if not data:
                return p
            if key.fileobj is p.stdout:
                print(data, end="")
            else:
                print(data, end="", file=sys.stderr)

def run_and_pipe(*args, **kwargs):
    if 'stdout' not in kwargs:
        kwargs['stdout'] = PIPE
    if 'stderr' not in kwargs:
        kwargs['stderr'] = PIPE
    p = popen(*args, **kwargs)
    return _print(p)

run_date_format = "%Y%m%dT%H%M%SZ"
def generate_date():
    return datetime.now().strftime(run_date_format)

def fixup_chain(repo, collection):
    butler = dafButler.Butler(repo, writeable=True)
    runs = butler.registry.queryCollections(re.compile(collection + "/.*/\d{8}T\d{6}Z$"), collectionTypes=CollectionType.RUN)
    existing_children = butler.registry.getCollectionChain(collection)
    other_children = [c for c in existing_children if c not in runs]
    runs = sorted(runs, key=lambda x : datetime.fromisoformat(re.compile(".*/(?P<date>\d{8}T\d{6}Z)$").match(x).groupdict()['date']), reverse=True)
    children = runs + other_children
    print("setting", collection, "=", children, file=sys.stderr)
    butler.registry.setCollectionChain(collection, children)

def normalize_pipeline(pipeline_path):
    filename = os.path.basename(pipeline_path)
    pipeline_name, pipeline_step = filename.split(".yaml")
    if pipeline_step == "":
        pipeline_step = "_"
    else:
        pipeline_step = pipeline_step.replace("#", "")
    return pipeline_name, pipeline_step


def construct_run(parent, pipeline_path):
    pipeline_name, pipeline_step = normalize_pipeline(pipeline_path)
    run = f"{parent}/{pipeline_name}/{pipeline_step}/{generate_date()}"
    return run

def should_run(repo, collection, pipeline, data_query=None, skip_existing=True, skip_failures=True):
    cmd = [
        "python",
        f"{os.path.dirname(__file__)}/qgraph.py",
        "-b", repo,
        "-i", collection,
        "--output-run", "dummy",
        "-p", pipeline,
    ]
    if data_query:
        cmd += ["-d", data_query]
    if skip_existing:
        cmd += ["--skip-existing-in", collection]
    if skip_failures:
        cmd += ["--skip-failures"]

    p = popen(cmd, stdout=PIPE, stderr=PIPE)
    stdout, stderr = p.communicate()
    print(stdout.decode())
    p.wait()
    if p.returncode != 0:
        for line in stderr.decode().split("\n"):
            if "quantum graph is empty" in line:
                return False
        raise RuntimeError("should run failure: " + stderr.decode())
    return True


def submit(repo, parent, pipeline_path, data_query=None, skip_existing=True, skip_failures=True, loop=False):
    
    fixup_chain(repo, parent)

    def inner():
        run = construct_run(parent, pipeline_path)
        qgraph_file = os.path.join(os.environ.get('TMPDIR', "/tmp"), run.replace("/", "_") + ".qgraph")
        cmd = [
            "pipetask",
            "--long-log", "--log-level", "VERBOSE",
            "qgraph",
            "-b", repo,
            "-p", pipeline_path,
            "-i", parent,
            "--output-run", run,
            "--save-qgraph", qgraph_file,
            "--qgraph-datastore-records", # for quantum-backed butler
        ]
        if data_query:
            cmd += ["-d", data_query]
        if skip_existing:
            cmd += ["--skip-existing-in", parent]

        p = run_and_pipe(cmd)
        p.wait()
        if p.returncode != 0:
            raise RuntimeError("quantum graph generation failed")
        
        cmd = [
            "bps", 
            "--long-log", "--log-level", "VERBOSE",
            "submit",
            f"{os.environ['PROC_LSST_DIR']}/pipelines/submit.yaml",
            # "--wms-service-class", "proc_lsst.shared.service.SharedParslService",
            "-b", repo,
            "-i", parent,
            "--output-run", run,
            "--qgraph", qgraph_file,
        ]
        p = run_and_pipe(cmd)
        p.wait()
        if p.returncode != 0:
            raise RuntimeError("bps submit failed")
        fixup_chain(repo, parent) # append run to chain

    check = lambda : should_run(repo, parent, pipeline_path, data_query=data_query, skip_existing=skip_existing, skip_failures=skip_failures)

    if loop:
        if skip_existing and skip_failures:
            # continue generating qgraph and running until there are no more failures
            while check():
                inner()
    else:
        # run once
        if (not skip_existing and not skip_failures) or check():
            inner()


def main():
    """
    Executes a pipeline on an input collection chain
    Constructs a run name by appending the pipeline name and the current date to the input collection
    Updates the collection chain to include the completed run
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("repo")
    parser.add_argument("parent")
    parser.add_argument("--pipeline")
    parser.add_argument("--where")
    parser.add_argument("--no-skip-existing", action="store_true")
    parser.add_argument("--no-skip-failures", action="store_true")
    parser.add_argument("--no-loop", action="store_true")

    args = parser.parse_args()
    # print(args)
    # return
    submit(
        args.repo, 
        args.parent, 
        args.pipeline, 
        data_query=args.where,
        skip_existing=not args.no_skip_existing,
        skip_failures=not args.no_skip_failures,
        loop=not args.no_loop,
    )

if __name__ == "__main__":
    main()
